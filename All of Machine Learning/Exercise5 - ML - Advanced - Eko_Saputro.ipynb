{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Python Machine Learning with PACMANN AI\n",
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Process Flowchart :\n",
    "### 1. Importing Data to Python \n",
    "    * Drop Duplicates \n",
    "### 2. Data Preprocessing :\n",
    "    * Input-Output Split, Train-Test Split\n",
    "    * Imputation, Processing Categorical, Normalization \n",
    "### 3. Training Machine Learning : \n",
    "    * Choose Score to optimize and Hyperparameter Space\n",
    "### 4. Test Prediction :\n",
    "    * Evaluate model performance on Test Data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Data to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Information\n",
    "\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\"). From this data we can analyze how travelers in February 2015 expressed their feelings on Twitter.\n",
    " \n",
    "Source : https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "### Content\n",
    "There are 13 variables: \n",
    "\n",
    "1. tweet_id\n",
    "2. airline_sentiment     : <b>Output</b>\n",
    "3. airline_sentiment_confidence\n",
    "4. negativereason\n",
    "5. negativereason_confidence\n",
    "6. airline\n",
    "7. name\n",
    "8. retweet_count\n",
    "9. text\n",
    "10. tweet_coord\n",
    "11. tweet_created\n",
    "12. tweet_location\n",
    "13. user_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baca dataset \n",
    "\n",
    "data = pd.read_csv('../datasets/tweet_airlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id airline_sentiment  \\\n",
       "0           0  570306133677760513           neutral   \n",
       "1           1  570301130888122368          positive   \n",
       "2           2  570301083672813571           neutral   \n",
       "3           3  570301031407624196          negative   \n",
       "4           4  570300817074462722          negative   \n",
       "\n",
       "   airline_sentiment_confidence negativereason  negativereason_confidence  \\\n",
       "0                        1.0000            NaN                        NaN   \n",
       "1                        0.3486            NaN                     0.0000   \n",
       "2                        0.6837            NaN                        NaN   \n",
       "3                        1.0000     Bad Flight                     0.7033   \n",
       "4                        1.0000     Can't Tell                     1.0000   \n",
       "\n",
       "          airline        name  retweet_count  \\\n",
       "0  Virgin America     cairdin              0   \n",
       "1  Virgin America    jnardino              0   \n",
       "2  Virgin America  yvonnalynn              0   \n",
       "3  Virgin America    jnardino              0   \n",
       "4  Virgin America    jnardino              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check 5 Observasi pertama dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Droping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek shape dari data yang akan di drop duplicate nya\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek jika ada atau tidak observasi yang duplikat\n",
    "\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop data yang duplikat\n",
    "\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek kembali shape\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id airline_sentiment  \\\n",
       "0           0  570306133677760513           neutral   \n",
       "1           1  570301130888122368          positive   \n",
       "2           2  570301083672813571           neutral   \n",
       "3           3  570301031407624196          negative   \n",
       "4           4  570300817074462722          negative   \n",
       "\n",
       "   airline_sentiment_confidence negativereason  negativereason_confidence  \\\n",
       "0                        1.0000            NaN                        NaN   \n",
       "1                        0.3486            NaN                     0.0000   \n",
       "2                        0.6837            NaN                        NaN   \n",
       "3                        1.0000     Bad Flight                     0.7033   \n",
       "4                        1.0000     Can't Tell                     1.0000   \n",
       "\n",
       "          airline        name  retweet_count  \\\n",
       "0  Virgin America     cairdin              0   \n",
       "1  Virgin America    jnardino              0   \n",
       "2  Virgin America  yvonnalynn              0   \n",
       "3  Virgin America    jnardino              0   \n",
       "4  Virgin America    jnardino              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek kembali data\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make function to import and drop \n",
    "\n",
    "Buat lah sebuah function dengan spesifikasi:\n",
    "\n",
    " 1. import data\n",
    " 2. cek JUMLAH OBSERVASI dan JUMLAH COLUMN\n",
    " 3. drop duplicate\n",
    " 4. drop unnecassary column\n",
    " 5. cek JUMLAH OBSERVASI dan JUMLAH COLUMN, setelah di-drop\n",
    " 6. return data setelah di-drop\n",
    "\n",
    "Function dinamakan dengan `import_data` dan menerima 2 argument yaitu:\n",
    "\n",
    " 1. `filepath`: Direktori dimana data tersimpan\n",
    " 2. `drop`    : Nama kolom yang ingin di hapus\n",
    " \n",
    "Lalu assign function tersebut pada suatu variabel yang dengan nama `data_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Buatlah function \n",
    "\n",
    "def importData(filepath, drop):\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(\"Data asli : %d Observasi, %d Kolom.\" %data.shape, '\\n')   \n",
    "    print(\"Kolom yang di drop :\", drop, '\\n')\n",
    "    data_drop = data.drop(drop, axis=1)\n",
    "    print(\"Banyaknya data duplicate :\", data_drop.duplicated().sum(), '\\n')\n",
    "    data_unique = data_drop.drop_duplicates()\n",
    "    print(\"Data setelah di drop : %d Observasi, %d Kolom.\" %data_unique.shape)\n",
    "\n",
    "    return data_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asli : 14640 Observasi, 14 Kolom. \n",
      "\n",
      "Kolom yang di drop : ['Unnamed: 0', 'tweet_id', 'name', 'tweet_location', 'user_timezone', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence'] \n",
      "\n",
      "Banyaknya data duplicate : 137 \n",
      "\n",
      "Data setelah di drop : 14503 Observasi, 6 Kolom.\n"
     ]
    }
   ],
   "source": [
    "# Assign fuction kepada variabel data\n",
    "\n",
    "drop = ['Unnamed: 0', 'tweet_id', 'name', 'tweet_location', 'user_timezone',\n",
    "       'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence']\n",
    "data_sentiment = importData(\"tweet_airlines.csv\", drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline  retweet_count  \\\n",
       "0           neutral  Virgin America              0   \n",
       "1          positive  Virgin America              0   \n",
       "2           neutral  Virgin America              0   \n",
       "3          negative  Virgin America              0   \n",
       "4          negative  Virgin America              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created  \n",
       "0  2015-02-24 11:35:52 -0800  \n",
       "1  2015-02-24 11:15:59 -0800  \n",
       "2  2015-02-24 11:15:48 -0800  \n",
       "3  2015-02-24 11:15:36 -0800  \n",
       "4  2015-02-24 11:14:45 -0800  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "### Input-Output Split\n",
    "\n",
    "Disini kita akan memisahkan kolom berdasarkan input dan output.\n",
    "\n",
    "Data yang digunakan untuk input akan dinamakan dengan `X`, sedangkan untuk output dengan `y`.\n",
    "\n",
    "Pada dataset ini, kita hanya perlu menggunakan kolom `airline_sentiment` sebagai output kita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment         airline  retweet_count  \\\n",
       "0           neutral  Virgin America              0   \n",
       "1          positive  Virgin America              0   \n",
       "2           neutral  Virgin America              0   \n",
       "3          negative  Virgin America              0   \n",
       "4          negative  Virgin America              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created  \n",
       "0  2015-02-24 11:35:52 -0800  \n",
       "1  2015-02-24 11:15:59 -0800  \n",
       "2  2015-02-24 11:15:48 -0800  \n",
       "3  2015-02-24 11:15:36 -0800  \n",
       "4  2015-02-24 11:14:45 -0800  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek data menggunakan head()\n",
    "\n",
    "data_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make function for input and output\n",
    "\n",
    "Buatlah sebuah function dengan kriteria dibawah ini:\n",
    "\n",
    "1. data_input\n",
    "2. data_output\n",
    "3. return data_input dan data_output\n",
    "* Tujuan dari pembuatan function adalah agar function ini dapat digunakan kembali di cases berbeda. \n",
    "\n",
    "Function dinamakan dengan `extract_input_output` dan menerima 2 argument yaitu:\n",
    "\n",
    "1. `data`        : Dataset yang ingin di split\n",
    "2. `output_column_name` : Nama kolom yang ingin di jadikan output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Buatlah function tersebut disini\n",
    "\n",
    "def extract_input_output(data, output_column_name):\n",
    "    y = data[output_column_name]\n",
    "    x = data.drop(output_column_name, axis=1)\n",
    "\n",
    "    return x, y \n",
    "\n",
    "# Assign hasil dari funtion tersebut kepada X, y.\n",
    "# X: data input\n",
    "# y: data output\n",
    "x, y =  extract_input_output(data_sentiment, 'airline_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline  retweet_count  \\\n",
       "0  Virgin America              0   \n",
       "1  Virgin America              0   \n",
       "2  Virgin America              0   \n",
       "3  Virgin America              0   \n",
       "4  Virgin America              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created  \n",
       "0  2015-02-24 11:35:52 -0800  \n",
       "1  2015-02-24 11:15:59 -0800  \n",
       "2  2015-02-24 11:15:48 -0800  \n",
       "3  2015-02-24 11:15:36 -0800  \n",
       "4  2015-02-24 11:14:45 -0800  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     neutral\n",
       "1    positive\n",
       "2     neutral\n",
       "3    negative\n",
       "4    negative\n",
       "Name: airline_sentiment, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "\n",
    "Pada bagian ini, X dan y akan dibagi menjadi 2 set yaitu training dan tes. Kita akan menggunakan function dari library Scikit Learn yaitu `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import function train_test_split dari library Scikit Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split Function\n",
    "1. x adalah input\n",
    "2. y adalah output\n",
    "3. test size = seberapa besar test, contoh 0.20 untuk 20% test dari data\n",
    "4. random state adalah kunci untuk random, harus disetting sama, misal random_state = 123\n",
    "5. Output: \n",
    "    * x_train = input dari data training\n",
    "    * x_test = input dari data test\n",
    "    * y_train = output dari training data\n",
    "    * y_test = output dari training data\n",
    "6. urutan dari x_train, x_test, y_train dan y_test tidak boleh terbalik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data input training : (11602, 5)\n",
      "Data input test : (2901, 5)\n",
      "Data output training : (11602,)\n",
      "Data output test : (2901,)\n"
     ]
    }
   ],
   "source": [
    "# Cek shape untuk tiap set (X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Data input training :\", x_train.shape)\n",
    "print(\"Data input test :\", x_test.shape)\n",
    "print(\"Data output training :\", y_train.shape)\n",
    "print(\"Data output test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>American</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir @sa_craig no, not helped one bit....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 15:42:21 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>United</td>\n",
       "      <td>0</td>\n",
       "      <td>@united yes please! I am newly married and try...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:19:32 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir three cheers to your Denver staf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 15:25:24 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>American</td>\n",
       "      <td>1</td>\n",
       "      <td>@AmericanAir @sarahzou translation: we don't r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 17:07:45 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Delta</td>\n",
       "      <td>0</td>\n",
       "      <td>@JetBlue sooo earlier i said i couldnt fly wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 04:51:15 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline  retweet_count  \\\n",
       "14270   American              0   \n",
       "542       United              0   \n",
       "5178   Southwest              0   \n",
       "14114   American              1   \n",
       "6840       Delta              0   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "14270  @AmericanAir @sa_craig no, not helped one bit....         NaN   \n",
       "542    @united yes please! I am newly married and try...         NaN   \n",
       "5178   @SouthwestAir three cheers to your Denver staf...         NaN   \n",
       "14114  @AmericanAir @sarahzou translation: we don't r...         NaN   \n",
       "6840   @JetBlue sooo earlier i said i couldnt fly wit...         NaN   \n",
       "\n",
       "                   tweet_created  \n",
       "14270  2015-02-22 15:42:21 -0800  \n",
       "542    2015-02-24 10:19:32 -0800  \n",
       "5178   2015-02-21 15:25:24 -0800  \n",
       "14114  2015-02-22 17:07:45 -0800  \n",
       "6840   2015-02-24 04:51:15 -0800  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_col.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump x_train columns\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(x_train.columns,'input_col.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Numerical and Categorical Data Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get numeric using ._get_numeric_data()\n",
    "\n",
    "x_train_num = x_train._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count\n",
       "14270              0\n",
       "542                0\n",
       "5178               0\n",
       "14114              1\n",
       "6840               0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the columns\n",
    "\n",
    "x_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unexpected numerical column if any \n",
    "\n",
    "# num_categorical = [...]\n",
    "# x_train_num = x_train_num.drop(num_categorical, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['numerical_col.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump numerical columns\n",
    "\n",
    "joblib.dump(x_train_num.columns,'numerical_col.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Categorical, drop numerical columns\n",
    "\n",
    "x_train_cat = x_train.drop(x_train_num.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sa_craig no, not helped one bit....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 15:42:21 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>United</td>\n",
       "      <td>@united yes please! I am newly married and try...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:19:32 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir three cheers to your Denver staf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 15:25:24 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sarahzou translation: we don't r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 17:07:45 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue sooo earlier i said i couldnt fly wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 04:51:15 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline                                               text  \\\n",
       "14270   American  @AmericanAir @sa_craig no, not helped one bit....   \n",
       "542       United  @united yes please! I am newly married and try...   \n",
       "5178   Southwest  @SouthwestAir three cheers to your Denver staf...   \n",
       "14114   American  @AmericanAir @sarahzou translation: we don't r...   \n",
       "6840       Delta  @JetBlue sooo earlier i said i couldnt fly wit...   \n",
       "\n",
       "      tweet_coord              tweet_created  \n",
       "14270         NaN  2015-02-22 15:42:21 -0800  \n",
       "542           NaN  2015-02-24 10:19:32 -0800  \n",
       "5178          NaN  2015-02-21 15:25:24 -0800  \n",
       "14114         NaN  2015-02-22 17:07:45 -0800  \n",
       "6840          NaN  2015-02-24 04:51:15 -0800  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top observations!\n",
    "\n",
    "x_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function for Separating Numerical and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Def a function that returns x_train numerical and x_train categorical\n",
    "\n",
    "def splitNumCat(data):\n",
    "    data_num = data._get_numeric_data()\n",
    "#    data_num = data_num.drop(num_categorical, axis = 1)\n",
    "    data_cat = data.drop(list(data_num.columns.values) , axis = 1)\n",
    "\n",
    "    return data_num, data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call function\n",
    "\n",
    "x_train_num, x_train_cat = splitNumCat(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count\n",
       "14270              0\n",
       "542                0\n",
       "5178               0\n",
       "14114              1\n",
       "6840               0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train numerical observations!\n",
    "\n",
    "x_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sa_craig no, not helped one bit....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 15:42:21 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>United</td>\n",
       "      <td>@united yes please! I am newly married and try...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:19:32 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir three cheers to your Denver staf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 15:25:24 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sarahzou translation: we don't r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 17:07:45 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue sooo earlier i said i couldnt fly wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 04:51:15 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline                                               text  \\\n",
       "14270   American  @AmericanAir @sa_craig no, not helped one bit....   \n",
       "542       United  @united yes please! I am newly married and try...   \n",
       "5178   Southwest  @SouthwestAir three cheers to your Denver staf...   \n",
       "14114   American  @AmericanAir @sarahzou translation: we don't r...   \n",
       "6840       Delta  @JetBlue sooo earlier i said i couldnt fly wit...   \n",
       "\n",
       "      tweet_coord              tweet_created  \n",
       "14270         NaN  2015-02-22 15:42:21 -0800  \n",
       "542           NaN  2015-02-24 10:19:32 -0800  \n",
       "5178          NaN  2015-02-21 15:25:24 -0800  \n",
       "14114         NaN  2015-02-22 17:07:45 -0800  \n",
       "6840          NaN  2015-02-24 04:51:15 -0800  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train categorical observations!\n",
    "\n",
    "x_train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    negative\n",
       "542      negative\n",
       "5178     positive\n",
       "14114    negative\n",
       "6840     positive\n",
       "Name: airline_sentiment, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the y_train observations!\n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Time, Coordinate, and Text from Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sa_craig no, not helped one bit....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 15:42:21 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>United</td>\n",
       "      <td>@united yes please! I am newly married and try...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:19:32 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir three cheers to your Denver staf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 15:25:24 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir @sarahzou translation: we don't r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 17:07:45 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue sooo earlier i said i couldnt fly wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 04:51:15 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline                                               text  \\\n",
       "14270   American  @AmericanAir @sa_craig no, not helped one bit....   \n",
       "542       United  @united yes please! I am newly married and try...   \n",
       "5178   Southwest  @SouthwestAir three cheers to your Denver staf...   \n",
       "14114   American  @AmericanAir @sarahzou translation: we don't r...   \n",
       "6840       Delta  @JetBlue sooo earlier i said i couldnt fly wit...   \n",
       "\n",
       "      tweet_coord              tweet_created  \n",
       "14270         NaN  2015-02-22 15:42:21 -0800  \n",
       "542           NaN  2015-02-24 10:19:32 -0800  \n",
       "5178          NaN  2015-02-21 15:25:24 -0800  \n",
       "14114         NaN  2015-02-22 17:07:45 -0800  \n",
       "6840          NaN  2015-02-24 04:51:15 -0800  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train categorical observations!\n",
    "\n",
    "x_train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract and assign to x_train_---\n",
    "\n",
    "x_train_time = x_train_cat['tweet_created']\n",
    "x_train_coord = x_train_cat['tweet_coord']\n",
    "x_train_text = x_train_cat['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop time, coord, and text from categorical data\n",
    "\n",
    "x_train_cat = x_train_cat.drop(['tweet_created', 'tweet_coord', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    2015-02-22 15:42:21 -0800\n",
       "542      2015-02-24 10:19:32 -0800\n",
       "5178     2015-02-21 15:25:24 -0800\n",
       "14114    2015-02-22 17:07:45 -0800\n",
       "6840     2015-02-24 04:51:15 -0800\n",
       "Name: tweet_created, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train time observations!\n",
    "\n",
    "x_train_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    NaN\n",
       "542      NaN\n",
       "5178     NaN\n",
       "14114    NaN\n",
       "6840     NaN\n",
       "Name: tweet_coord, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train coordinate observations!\n",
    "\n",
    "x_train_coord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    @AmericanAir @sa_craig no, not helped one bit....\n",
       "542      @united yes please! I am newly married and try...\n",
       "5178     @SouthwestAir three cheers to your Denver staf...\n",
       "14114    @AmericanAir @sarahzou translation: we don't r...\n",
       "6840     @JetBlue sooo earlier i said i couldnt fly wit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train text observations!\n",
    "\n",
    "x_train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>Southwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline\n",
       "14270   American\n",
       "542       United\n",
       "5178   Southwest\n",
       "14114   American\n",
       "6840       Delta"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top of the x_train categorical observations!\n",
    "\n",
    "x_train_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['categorical_col.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump categorical columns\n",
    "\n",
    "joblib.dump(x_train_cat.columns, 'categorical_col.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation adalah proses pengisian data yang memiliki data yang kosong, biasanya diperlihatkan sebagai NaN\n",
    "\n",
    "Proses tersebut terbagi menjadi 2:\n",
    "* Numerical Imputation\n",
    "* Categorical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline              0\n",
       "retweet_count        0\n",
       "text                 0\n",
       "tweet_coord      10798\n",
       "tweet_created        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cek data yang kosong di traininig set input\n",
    "\n",
    "x_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the missing value of the x_train_num\n",
    "\n",
    "x_train_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a function for numerical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Imputer\n",
    "\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function only to fit Imputer\n",
    "#\n",
    "# input argument : data, missing_values, strategy\n",
    "#\n",
    "# return fitted Imputer\n",
    "\n",
    "def fitImputNum(data, missing_values, strategy):\n",
    "    # define \n",
    "    imput = Imputer(missing_values, strategy)\n",
    "\n",
    "    # fit\n",
    "    imput.fit(data)\n",
    "\n",
    "    return imput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call function for fitting Imputer\n",
    "\n",
    "imputer = fitImputNum(x_train_num, 'NaN', 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imputer.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump imputer\n",
    "\n",
    "joblib.dump(imputer, 'imputer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to transform Numerical data using Imputer\n",
    "#\n",
    "# input argument : data, imputer\n",
    "#\n",
    "# return data_num_imputed\n",
    "\n",
    "def transformNumerical(data, imputer):\n",
    "    # transform\n",
    "    data_num_imputed = pd.DataFrame(imputer.transform(data))\n",
    "\n",
    "    # replace broken column and index\n",
    "    data_num_imputed.columns = data.columns\n",
    "    data_num_imputed.index = data.index\n",
    "\n",
    "    return data_num_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call function for transform Numerical data\n",
    "\n",
    "x_train_num_imputed = transformNumerical(x_train_num, imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "retweet_count    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the missing value of the imputed data\n",
    "\n",
    "x_train_num_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Make a function for categorical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function definition, return imputed data\n",
    "\n",
    "def categoricalImputation(data):\n",
    "    # fillna\n",
    "    data_cat_imputed = data.fillna(value='KOSONG')\n",
    "    \n",
    "    return data_cat_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call function for imputation\n",
    "\n",
    "x_train_cat_imputed = categoricalImputation(x_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the missing value of the imputed data\n",
    "\n",
    "x_train_cat_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function to get the dummies using Label Encoder & Label Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import LabelBinarizer and LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funtion definition\n",
    "\n",
    "def categoricalDummies(data):\n",
    "    dummy_variables = pd.DataFrame([])\n",
    "    label_encoder = pd.Series([])\n",
    "    label_binarizer = pd.Series([])\n",
    "    \n",
    "    j = 0\n",
    "    for i in list(data):\n",
    "        label_en = LabelEncoder()\n",
    "        label_bin = LabelBinarizer()\n",
    "        \n",
    "        encoded = label_en.fit_transform(data[i])\n",
    "        binary = label_bin.fit_transform(encoded)\n",
    "        \n",
    "        dummy = pd.DataFrame(binary, columns=[\"{}_{}\".format(a, b) for b in sorted(data[i].unique())\n",
    "                                              for a in [i]], \n",
    "                             index = data.index)\n",
    "        dummy_variables = pd.concat([dummy_variables, dummy], axis = 1)\n",
    "        label_encoder[j] = label_en\n",
    "        label_binarizer[j] = label_bin\n",
    "        \n",
    "        j += 1\n",
    "    dummy_columns = dummy_variables.columns\n",
    "    \n",
    "    return dummy_variables, label_encoder, label_binarizer,dummy_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call categoricalDummies\n",
    "\n",
    "x_train_cat_imputed_dummy, label_encoder, label_binarizer, dummy_columns = categoricalDummies(x_train_cat_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dummy_columns.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump dummy_columns\n",
    "\n",
    "joblib.dump(dummy_columns, 'dummy_columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump label_encoder\n",
    "\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_binarizer.pkl']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump label_binarizer\n",
    "\n",
    "joblib.dump(label_binarizer, 'label_binarizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_American</th>\n",
       "      <th>airline_Delta</th>\n",
       "      <th>airline_Southwest</th>\n",
       "      <th>airline_US Airways</th>\n",
       "      <th>airline_United</th>\n",
       "      <th>airline_Virgin America</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_American  airline_Delta  airline_Southwest  airline_US Airways  \\\n",
       "14270                 1              0                  0                   0   \n",
       "542                   0              0                  0                   0   \n",
       "5178                  0              0                  1                   0   \n",
       "14114                 1              0                  0                   0   \n",
       "6840                  0              1                  0                   0   \n",
       "\n",
       "       airline_United  airline_Virgin America  \n",
       "14270               0                       0  \n",
       "542                 1                       0  \n",
       "5178                0                       0  \n",
       "14114               0                       0  \n",
       "6840                0                       0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the top observations\n",
    "\n",
    "x_train_cat_imputed_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    9501\n",
      "1    2101\n",
      "Name: airline_American, dtype: int64 \n",
      "\n",
      "\n",
      "0    9830\n",
      "1    1772\n",
      "Name: airline_Delta, dtype: int64 \n",
      "\n",
      "\n",
      "0    9684\n",
      "1    1918\n",
      "Name: airline_Southwest, dtype: int64 \n",
      "\n",
      "\n",
      "0    9265\n",
      "1    2337\n",
      "Name: airline_US Airways, dtype: int64 \n",
      "\n",
      "\n",
      "0    8532\n",
      "1    3070\n",
      "Name: airline_United, dtype: int64 \n",
      "\n",
      "\n",
      "0    11198\n",
      "1      404\n",
      "Name: airline_Virgin America, dtype: int64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in x_train_cat_imputed_dummy.columns:\n",
    "    print(x_train_cat_imputed_dummy[i].value_counts(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Time, Coordinate, and Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunakan program pada Exercise 4 untuk melakukan preprocessing time, coordinate, dan text. <br>\n",
    "Lakukan modifikasi agar program terdiri dari fungsi-fungsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def columnSplit(data, splitter, columns_name)\n",
    "#\n",
    "# return data_splitted\n",
    "\n",
    "def columnSplit(data, splitter, columns_name):\n",
    "    data_splitted = pd.DataFrame(data.str.split(splitter).tolist(),\n",
    "                        columns = columns_name,\n",
    "                        index = data.index)\n",
    "\n",
    "    return data_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    2015-02-22 15:42:21 -0800\n",
       "542      2015-02-24 10:19:32 -0800\n",
       "5178     2015-02-21 15:25:24 -0800\n",
       "14114    2015-02-22 17:07:45 -0800\n",
       "6840     2015-02-24 04:51:15 -0800\n",
       "Name: tweet_created, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocessTime(data)\n",
    "#\n",
    "# return data_time (only return necessary column! read the dataset information carefully)\n",
    "# \n",
    "def preprocessTime(data):\n",
    "    # columSplit\n",
    "    data_time_full = columnSplit(data, ' ', ['date', 'time', 'GMT'])\n",
    "\n",
    "    # columSplit again\n",
    "    data_date = columnSplit(data_time_full['date'], '-', ['year', 'month', 'day'])\n",
    "\n",
    "    # and again\n",
    "    data_hour = columnSplit(data_time_full['time'], ':', ['hour', 'minute', 'second'])\n",
    "\n",
    "    \n",
    "    # only return necessary column!\n",
    "    data_time_clean = pd.concat([data_date['day'], data_hour], axis=1)\n",
    "    \n",
    "    return data_time_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call preprocessTime\n",
    "\n",
    "x_train_time_clean = preprocessTime(x_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in x_train_time_clean:\n",
    "    x_train_time_clean[i] = x_train_time_clean[i].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       day  hour  minute  second\n",
       "14270   22    15      42      21\n",
       "542     24    10      19      32\n",
       "5178    21    15      25      24\n",
       "14114   22    17       7      45\n",
       "6840    24     4      51      15"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation of time data\n",
    "\n",
    "x_train_time_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocessCoordinate(data)\n",
    "#\n",
    "# return data_coordinate\n",
    "#\n",
    "def preprocessCoordinate(data):\n",
    "    # fillna\n",
    "    data_coord = data.fillna(value = '[0.0, 0.0]')\n",
    "\n",
    "    # get value\n",
    "    data_coord = data_coord.str[1:-1]\n",
    "\n",
    "    # column split\n",
    "    data_coordinate = columnSplit(data_coord, ',', ['latitude', 'longitude'])\n",
    "\n",
    "    # to_numeric\n",
    "    for i in list(data_coordinate):\n",
    "        data_coordinate[i] = pd.to_numeric(data_coordinate[i])\n",
    "        \n",
    "    return data_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call preprocessCoordinate\n",
    "\n",
    "x_train_coord_clean = preprocessCoordinate(x_train_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude  longitude\n",
       "14270       0.0        0.0\n",
       "542         0.0        0.0\n",
       "5178        0.0        0.0\n",
       "14114       0.0        0.0\n",
       "6840        0.0        0.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation of coordinate data\n",
    "\n",
    "x_train_coord_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>======================================================================</font><br>\n",
    "Sebelum melakukan \"Stemming and Lemmatization\", ekstrak isi nltk_data.rar ke Drive C:<br>\n",
    "Pastikan directory C:\\nltk_data\\corpora berisi folder wordnet dan file wordnet.zip\n",
    "<font color='red'>======================================================================</font><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re, SnowballStemmer, WordnetLemmatizer\n",
    "\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define stemmer and lemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocessText(text, stemmer, lemmatizer)\n",
    "#\n",
    "# return clear_text\n",
    "#\n",
    "def preprocessText(text, stemmer, lemmatizer):\n",
    "    clear_text = pd.Series([])\n",
    "    \n",
    "    for i  in text.index:\n",
    "        string = str(text[i])\n",
    "        \n",
    "        # Preprocess using RegularExpression\n",
    "        string = re.sub('[^A-Za-z0-9]+', ' ', string)\n",
    "        string = re.sub(' +', ' ', string.strip())\n",
    "        string.lower()\n",
    "        \n",
    "        # Stemming\n",
    "        string = str(string)\n",
    "        string = string.split(\" \")\n",
    "        string = [stemmer.stem(word) for word in string]\n",
    "        string = \" \".join(string)\n",
    "\n",
    "    \n",
    "        # Lemmatizing\n",
    "        string = str(string)\n",
    "        string = string.split(\" \")\n",
    "        string = [lemmatizer.lemmatize(word) for word in string]\n",
    "        string = \" \".join(string)\n",
    "    \n",
    "        # Save to clear_text[i]\n",
    "        clear_text[i] = string\n",
    "\n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call preprocessText\n",
    "\n",
    "x_train_text_clean = preprocessText(x_train_text, stemmer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270    americanair sa craig no not help one bit actua...\n",
       "542      unit yes plea i am newli marri and tri to upda...\n",
       "5178     southwestair three cheer to your denver staff ...\n",
       "14114    americanair sarahzou translat we don t reinves...\n",
       "6840     jetblu sooo earlier i said i couldnt fli with ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation of text data\n",
    "\n",
    "x_train_text_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import TF-IDF Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=500, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fitVectorizer(text, vectorizer)\n",
    "#\n",
    "# return fitted vectorizer \n",
    "#\n",
    "# only fit the vectorizer\n",
    "\n",
    "def fitVectorizer(text, vectorizer):\n",
    "    #fit the vectorizer\n",
    "    vectorizer.fit(text)\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def transformText(text, fitted_vectorizer)\n",
    "#\n",
    "# return feature_word\n",
    "\n",
    "def transformText(text, vectorizer):\n",
    "    # transform using vectorizer\n",
    "    tf_idf = vectorizer.transform(text)\n",
    "\n",
    "    # make feature_word\n",
    "    feature_word = pd.DataFrame(tf_idf.toarray(),\n",
    "                               columns=vectorizer.get_feature_names(),\n",
    "                               index=text.index)\n",
    "\n",
    "    return feature_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call fitVectorizer\n",
    "\n",
    "vectorizer = fitVectorizer(x_train_text_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump vectorizer\n",
    "\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call transformText\n",
    "\n",
    "x_train_text_feature = transformText(x_train_text_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americanair</th>\n",
       "      <th>bag</th>\n",
       "      <th>cancel</th>\n",
       "      <th>custom</th>\n",
       "      <th>delay</th>\n",
       "      <th>fli</th>\n",
       "      <th>flight</th>\n",
       "      <th>help</th>\n",
       "      <th>hold</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>need</th>\n",
       "      <th>plane</th>\n",
       "      <th>servic</th>\n",
       "      <th>southwestair</th>\n",
       "      <th>thank</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "      <th>usairway</th>\n",
       "      <th>wa</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>0.581908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.813254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.62008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>0.446839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.815043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       americanair  bag  cancel   custom  delay       fli    flight      help  \\\n",
       "14270     0.581908  0.0     0.0  0.00000    0.0  0.000000  0.000000  0.813254   \n",
       "542       0.000000  0.0     0.0  0.00000    0.0  0.000000  0.707205  0.000000   \n",
       "5178      0.000000  0.0     0.0  0.00000    0.0  0.000000  0.000000  0.000000   \n",
       "14114     0.446839  0.0     0.0  0.63784    0.0  0.000000  0.000000  0.000000   \n",
       "6840      0.000000  0.0     0.0  0.00000    0.0  0.815043  0.000000  0.000000   \n",
       "\n",
       "       hold  hour  ...   need  plane    servic  southwestair  thank  time  \\\n",
       "14270   0.0   0.0  ...    0.0    0.0  0.000000       0.00000    0.0   0.0   \n",
       "542     0.0   0.0  ...    0.0    0.0  0.000000       0.00000    0.0   0.0   \n",
       "5178    0.0   0.0  ...    0.0    0.0  0.000000       0.62008    0.0   0.0   \n",
       "14114   0.0   0.0  ...    0.0    0.0  0.627292       0.00000    0.0   0.0   \n",
       "6840    0.0   0.0  ...    0.0    0.0  0.000000       0.00000    0.0   0.0   \n",
       "\n",
       "           unit  usairway   wa  wait  \n",
       "14270  0.000000       0.0  0.0   0.0  \n",
       "542    0.707009       0.0  0.0   0.0  \n",
       "5178   0.000000       0.0  0.0   0.0  \n",
       "14114  0.000000       0.0  0.0   0.0  \n",
       "6840   0.000000       0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation of feature_word\n",
    "\n",
    "x_train_text_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ambil variabel numerical yang sudah tidak memiliki missing values, variabel kategori yang sudah menjadi dummy,\n",
    "# variabel time, coord,dan text yang sudah di preprocess\n",
    "\n",
    "# satukan kembali kolom tersebut menjadi x_train_concat\n",
    "x_train_concat = pd.concat([x_train_num_imputed, x_train_time_clean, \n",
    "                            x_train_coord_clean, x_train_text_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>americanair</th>\n",
       "      <th>bag</th>\n",
       "      <th>cancel</th>\n",
       "      <th>...</th>\n",
       "      <th>need</th>\n",
       "      <th>plane</th>\n",
       "      <th>servic</th>\n",
       "      <th>southwestair</th>\n",
       "      <th>thank</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "      <th>usairway</th>\n",
       "      <th>wa</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.62008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count  day  hour  minute  second  latitude  longitude  \\\n",
       "14270            0.0   22    15      42      21       0.0        0.0   \n",
       "542              0.0   24    10      19      32       0.0        0.0   \n",
       "5178             0.0   21    15      25      24       0.0        0.0   \n",
       "14114            1.0   22    17       7      45       0.0        0.0   \n",
       "6840             0.0   24     4      51      15       0.0        0.0   \n",
       "\n",
       "       americanair  bag  cancel  ...   need  plane    servic  southwestair  \\\n",
       "14270     0.581908  0.0     0.0  ...    0.0    0.0  0.000000       0.00000   \n",
       "542       0.000000  0.0     0.0  ...    0.0    0.0  0.000000       0.00000   \n",
       "5178      0.000000  0.0     0.0  ...    0.0    0.0  0.000000       0.62008   \n",
       "14114     0.446839  0.0     0.0  ...    0.0    0.0  0.627292       0.00000   \n",
       "6840      0.000000  0.0     0.0  ...    0.0    0.0  0.000000       0.00000   \n",
       "\n",
       "       thank  time      unit  usairway   wa  wait  \n",
       "14270    0.0   0.0  0.000000       0.0  0.0   0.0  \n",
       "542      0.0   0.0  0.707009       0.0  0.0   0.0  \n",
       "5178     0.0   0.0  0.000000       0.0  0.0   0.0  \n",
       "14114    0.0   0.0  0.000000       0.0  0.0   0.0  \n",
       "6840     0.0   0.0  0.000000       0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation\n",
    "\n",
    "x_train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NaN values\n",
    "\n",
    "x_train_concat.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Variables\n",
    "\n",
    "- KEGUNAAN: Menyamakan skala dari variable input\n",
    "- fit: imputer agar mengetahui mean standard deviasi dari setiap column\n",
    "- transform: isi data dengan value yang dinormalisasi\n",
    "- output dari transform berupda pd dataframe\n",
    "- normalize dikeluarkan karena akan dipakai di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Standard Scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fitStandardize(data)\n",
    "#\n",
    "# return fitted standardizer\n",
    "\n",
    "def fitStandardize(data):\n",
    "    #define standardizer\n",
    "    standardizer = StandardScaler()\n",
    "\n",
    "    #fit\n",
    "    standard = standardizer.fit(data)\n",
    "\n",
    "    return standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def transformStandardize(data, standardizer)\n",
    "#\n",
    "# return standardized_data\n",
    "def transformStandardize(data, standardizer):\n",
    "    # transform data\n",
    "    data_standard = pd.DataFrame(standardizer.transform(data))\n",
    "\n",
    "    # replace broken column and index\n",
    "    data_standard.columns = data.columns\n",
    "    data_standard.index = data.index\n",
    "\n",
    "    return data_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call fitStandardize\n",
    "\n",
    "normalizer = fitStandardize(x_train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standardizer.pkl']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump standardizer\n",
    "\n",
    "joblib.dump(normalizer, 'standardizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call transformStandardize\n",
    "\n",
    "x_train_standardize = transformStandardize(x_train_concat, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>americanair</th>\n",
       "      <th>bag</th>\n",
       "      <th>cancel</th>\n",
       "      <th>...</th>\n",
       "      <th>need</th>\n",
       "      <th>plane</th>\n",
       "      <th>servic</th>\n",
       "      <th>southwestair</th>\n",
       "      <th>thank</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "      <th>usairway</th>\n",
       "      <th>wa</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>0.736408</td>\n",
       "      <td>-0.479603</td>\n",
       "      <td>-0.241366</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>1.744146</td>\n",
       "      <td>-0.218851</td>\n",
       "      <td>-0.265185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>-0.108057</td>\n",
       "      <td>1.423103</td>\n",
       "      <td>-0.436926</td>\n",
       "      <td>-0.600032</td>\n",
       "      <td>0.154485</td>\n",
       "      <td>-0.241366</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>-0.451864</td>\n",
       "      <td>-0.218851</td>\n",
       "      <td>-0.265185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>1.810427</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>-0.251396</td>\n",
       "      <td>-0.306670</td>\n",
       "      <td>-0.241366</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>-0.451864</td>\n",
       "      <td>-0.218851</td>\n",
       "      <td>-0.265185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>1.937934</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1.248735</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.875760</td>\n",
       "      <td>-1.297305</td>\n",
       "      <td>0.903861</td>\n",
       "      <td>-0.241366</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>1.234422</td>\n",
       "      <td>-0.218851</td>\n",
       "      <td>-0.265185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>3.907741</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>-0.108057</td>\n",
       "      <td>1.423103</td>\n",
       "      <td>-1.562086</td>\n",
       "      <td>1.259362</td>\n",
       "      <td>-0.825469</td>\n",
       "      <td>-0.241366</td>\n",
       "      <td>0.230431</td>\n",
       "      <td>-0.451864</td>\n",
       "      <td>-0.218851</td>\n",
       "      <td>-0.265185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count       day      hour    minute    second  latitude  \\\n",
       "14270      -0.108057  0.499135  0.500707  0.736408 -0.479603 -0.241366   \n",
       "542        -0.108057  1.423103 -0.436926 -0.600032  0.154485 -0.241366   \n",
       "5178       -0.108057  0.037151  0.500707 -0.251396 -0.306670 -0.241366   \n",
       "14114       1.248735  0.499135  0.875760 -1.297305  0.903861 -0.241366   \n",
       "6840       -0.108057  1.423103 -1.562086  1.259362 -0.825469 -0.241366   \n",
       "\n",
       "       longitude  americanair       bag    cancel    ...         need  \\\n",
       "14270   0.230431     1.744146 -0.218851 -0.265185    ...    -0.214438   \n",
       "542     0.230431    -0.451864 -0.218851 -0.265185    ...    -0.214438   \n",
       "5178    0.230431    -0.451864 -0.218851 -0.265185    ...    -0.214438   \n",
       "14114   0.230431     1.234422 -0.218851 -0.265185    ...    -0.214438   \n",
       "6840    0.230431    -0.451864 -0.218851 -0.265185    ...    -0.214438   \n",
       "\n",
       "          plane    servic  southwestair     thank      time      unit  \\\n",
       "14270 -0.218052 -0.261428     -0.416098 -0.343852 -0.253241 -0.543325   \n",
       "542   -0.218052 -0.261428     -0.416098 -0.343852 -0.253241  1.810427   \n",
       "5178  -0.218052 -0.261428      1.937934 -0.343852 -0.253241 -0.543325   \n",
       "14114 -0.218052  3.907741     -0.416098 -0.343852 -0.253241 -0.543325   \n",
       "6840  -0.218052 -0.261428     -0.416098 -0.343852 -0.253241 -0.543325   \n",
       "\n",
       "       usairway        wa      wait  \n",
       "14270 -0.464249 -0.312419 -0.221933  \n",
       "542   -0.464249 -0.312419 -0.221933  \n",
       "5178  -0.464249 -0.312419 -0.221933  \n",
       "14114 -0.464249 -0.312419 -0.221933  \n",
       "6840  -0.464249 -0.312419 -0.221933  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation\n",
    "\n",
    "x_train_standardize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setelah input numerical distandarisasi, kita gabungkan dengan dummy variables\n",
    "\n",
    "x_train_clean = pd.concat([x_train_cat_imputed_dummy, x_train_standardize], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clean.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_American</th>\n",
       "      <th>airline_Delta</th>\n",
       "      <th>airline_Southwest</th>\n",
       "      <th>airline_US Airways</th>\n",
       "      <th>airline_United</th>\n",
       "      <th>airline_Virgin America</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>...</th>\n",
       "      <th>need</th>\n",
       "      <th>plane</th>\n",
       "      <th>servic</th>\n",
       "      <th>southwestair</th>\n",
       "      <th>thank</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "      <th>usairway</th>\n",
       "      <th>wa</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14270</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>0.736408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>1.423103</td>\n",
       "      <td>-0.436926</td>\n",
       "      <td>-0.600032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>1.810427</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.037151</td>\n",
       "      <td>0.500707</td>\n",
       "      <td>-0.251396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>1.937934</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14114</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.248735</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.875760</td>\n",
       "      <td>-1.297305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>3.907741</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>1.423103</td>\n",
       "      <td>-1.562086</td>\n",
       "      <td>1.259362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_American  airline_Delta  airline_Southwest  airline_US Airways  \\\n",
       "14270                 1              0                  0                   0   \n",
       "542                   0              0                  0                   0   \n",
       "5178                  0              0                  1                   0   \n",
       "14114                 1              0                  0                   0   \n",
       "6840                  0              1                  0                   0   \n",
       "\n",
       "       airline_United  airline_Virgin America  retweet_count       day  \\\n",
       "14270               0                       0      -0.108057  0.499135   \n",
       "542                 1                       0      -0.108057  1.423103   \n",
       "5178                0                       0      -0.108057  0.037151   \n",
       "14114               0                       0       1.248735  0.499135   \n",
       "6840                0                       0      -0.108057  1.423103   \n",
       "\n",
       "           hour    minute    ...         need     plane    servic  \\\n",
       "14270  0.500707  0.736408    ...    -0.214438 -0.218052 -0.261428   \n",
       "542   -0.436926 -0.600032    ...    -0.214438 -0.218052 -0.261428   \n",
       "5178   0.500707 -0.251396    ...    -0.214438 -0.218052 -0.261428   \n",
       "14114  0.875760 -1.297305    ...    -0.214438 -0.218052  3.907741   \n",
       "6840  -1.562086  1.259362    ...    -0.214438 -0.218052 -0.261428   \n",
       "\n",
       "       southwestair     thank      time      unit  usairway        wa  \\\n",
       "14270     -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "542       -0.416098 -0.343852 -0.253241  1.810427 -0.464249 -0.312419   \n",
       "5178       1.937934 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "14114     -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "6840      -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "\n",
       "           wait  \n",
       "14270 -0.221933  \n",
       "542   -0.221933  \n",
       "5178  -0.221933  \n",
       "14114 -0.221933  \n",
       "6840  -0.221933  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Machine Learning\n",
    "* Kita harus mengalahkan benchmark\n",
    "* Choose Score to optimize and Hyperparameter Space\n",
    "* Cross-Validation: Random Search CV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    0.626875\n",
       "neutral     0.212981\n",
       "positive    0.160145\n",
       "Name: airline_sentiment, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decTree_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    decTree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "    hyperparam = {'min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'max_features': ['sqrt', 'log2', 0.25, 0.5, 0.75]}\n",
    "\n",
    "    random_decTree = RandomizedSearchCV(decTree, param_distributions = hyperparam, cv = 5,\n",
    "                                        n_iter = 10, scoring = scoring, n_jobs=2, random_state = 123)\n",
    "    \n",
    "    random_decTree.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_decTree.best_score_)\n",
    "    print (\"Best Param\", random_decTree.best_params_)\n",
    "    \n",
    "    return random_decTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.680055162903\n",
      "Best Param {'min_samples_leaf': 60, 'max_features': 0.75}\n"
     ]
    }
   ],
   "source": [
    "best_decTree = decTree_fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=0.75, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=60,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=123, splitter='best')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decTree = DecisionTreeClassifier(min_samples_leaf = best_decTree.best_params_.get('min_samples_leaf'),\n",
    "                                 max_features = best_decTree.best_params_.get('max_features'), random_state=123)\n",
    "decTree.fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def bagging_fit(x_train, y_train, scoring = 'accuracy')\n",
    "#\n",
    "#\n",
    "# return model\n",
    "\n",
    "def bagging_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    #define DecisionTreeClassifier\n",
    "    dtree = DecisionTreeClassifier(random_state=123)\n",
    "                                      \n",
    "    #define bagging and use DecisionTree as base estimator\n",
    "    bagging = BaggingClassifier(base_estimator = dtree, random_state=123)\n",
    "                                      \n",
    "    # define hyperparameter \n",
    "    hyperparam = {'base_estimator__min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'n_estimators': [100, 200, 300, 500, 1000]}\n",
    "    # 'base_estimator__' sebelum 'min_samples_leaf' menandakan hyperparameter yang dicari ada di dalam base estimatornya\n",
    "    # dalam hal ini berarti decTree\n",
    "    # (min_samples_leaf ada di dalam decTree)\n",
    "    \n",
    "    # do randomizedsearchCV for bagging, set the scoring on randomizedsearch\n",
    "    random_bagging = RandomizedSearchCV(bagging, param_distributions = hyperparam, cv = 5,\n",
    "                                    n_iter = 10, n_jobs=2, random_state = 123)\n",
    "      \n",
    "    # fit\n",
    "    random_bagging.fit(x_train, y_train)\n",
    "\n",
    "                                        \n",
    "    print (\"Best Accuracy\", random_bagging.best_score_)\n",
    "    print (\"Best Param\", random_bagging.best_params_)\n",
    "   \n",
    "    return random_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.693932080676\n",
      "Best Param {'n_estimators': 100, 'base_estimator__min_samples_leaf': 33}\n"
     ]
    }
   ],
   "source": [
    "# call bagging_fit function\n",
    "\n",
    "best_bagging = bagging_fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=33,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=123, splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=100, n_jobs=2, oob_score=False,\n",
       "         random_state=123, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model with best_params\n",
    "\n",
    "decTreeBag = DecisionTreeClassifier(min_samples_leaf = best_bagging.best_params_.get('base_estimator__min_samples_leaf'),\n",
    "                                    random_state=123)\n",
    "bagging = BaggingClassifier(base_estimator = decTreeBag, \n",
    "                            n_estimators = best_bagging.best_params_.get('n_estimators'),\n",
    "                            random_state=123, n_jobs=2)\n",
    "bagging.fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def randomForest_fit(x_train, y_train, scoring = 'accuracy')\n",
    "#\n",
    "#\n",
    "# return model\n",
    "\n",
    "def randomForest_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    # define classifier\n",
    "    randomForest = RandomForestClassifier(random_state=123)\n",
    "                                          \n",
    "    # define hyperparameter \n",
    "    hyperparam = {'min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'max_features': ['sqrt', 'log2', 0.25, 0.5, 0.75], \n",
    "                  'n_estimators': [100, 200, 300, 500, 1000]}\n",
    "                                          \n",
    "    # do randomizedsearchCV for random forest, set the scoring on randomizedsearch\n",
    "    random_randomForest = RandomizedSearchCV(randomForest, param_distributions=hyperparam,cv = 5,\n",
    "                                             n_iter = 10, n_jobs=2, random_state = 123)\n",
    "    \n",
    "    # fit\n",
    "    random_randomForest.fit(x_train, y_train)\n",
    "    \n",
    "        \n",
    "    print (\"Best Accuracy\", random_randomForest.best_score_)\n",
    "    print (\"Best Param\", random_randomForest.best_params_)\n",
    "        \n",
    "    return random_randomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.697034993967\n",
      "Best Param {'n_estimators': 100, 'min_samples_leaf': 5, 'max_features': 'log2'}\n"
     ]
    }
   ],
   "source": [
    "# call randomForest_fit function\n",
    "\n",
    "best_randForest = randomForest_fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=5,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model with best_params\n",
    "\n",
    "randForest = RandomForestClassifier(min_samples_leaf=best_randForest.best_params_.get('min_samples_leaf'),\n",
    "                                   max_features=best_randForest.best_params_.get('max_features'),\n",
    "                                   n_estimators=best_randForest.best_params_.get('n_estimators'))\n",
    "    \n",
    "randForest.fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def adaBoost_fit(x_train, y_train, scoring = 'accuracy')\n",
    "#\n",
    "#\n",
    "# return model\n",
    "\n",
    "def adaBoost_fit(x_train, y_train, scoring = 'accuracy'):\n",
    "    #define DecisionTreeClassifier\n",
    "    dtree = DecisionTreeClassifier(random_state=123)\n",
    "    \n",
    "    #define bagging and use DecisionTree as base estimator\n",
    "    adaBoost = AdaBoostClassifier(base_estimator = dtree , random_state=123)\n",
    "    \n",
    "    # define hyperparameter \n",
    "    hyperparam = {'base_estimator__min_samples_leaf': [3, 5, 7, 9, 13, 17, 21, 27, 33, 41, 50, 60, 80, 100],\n",
    "                  'learning_rate':[1., .1, .01, .001],\n",
    "                  'n_estimators': [100, 200, 300, 500, 1000]}\n",
    "    \n",
    "    # do randomizedsearchCV for random forest, set the scoring on randomizedsearch\n",
    "    random_adaBoost = RandomizedSearchCV(adaBoost, param_distributions = hyperparam, cv = 5,\n",
    "                                    n_iter =10, n_jobs=2, random_state = 123)\n",
    "        \n",
    "    # fit\n",
    "    random_adaBoost.fit(x_train, y_train)\n",
    "    \n",
    "    print (\"Best Accuracy\", random_adaBoost.best_score_)\n",
    "    print (\"Best Param\", random_adaBoost.best_params_)\n",
    "    \n",
    "    return random_adaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy 0.686433373556\n",
      "Best Param {'n_estimators': 300, 'learning_rate': 0.001, 'base_estimator__min_samples_leaf': 80}\n"
     ]
    }
   ],
   "source": [
    "# call adaBoost_fit\n",
    "\n",
    "best_adaBoost = adaBoost_fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=80,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=123, splitter='best'),\n",
       "          learning_rate=0.001, n_estimators=300, random_state=123)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model with best_params\n",
    "\n",
    "decTreeAdaBoost = DecisionTreeClassifier(min_samples_leaf = best_adaBoost.best_params_.get('base_estimator__min_samples_leaf'),\n",
    "                                    random_state=123)\n",
    "adaBoost = AdaBoostClassifier(base_estimator = decTreeAdaBoost, \n",
    "                              n_estimators = best_adaBoost.best_params_.get('n_estimators'),\n",
    "                              learning_rate = best_adaBoost.best_params_.get('learning_rate'),\n",
    "                              random_state=123)\n",
    "    \n",
    "adaBoost.fit(x_train_clean, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adaBoost.pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump classifier\n",
    "\n",
    "joblib.dump(decTree, 'decTree.pkl')\n",
    "joblib.dump(bagging,'bagging.pkl')\n",
    "joblib.dump(randForest,'randForest.pkl')\n",
    "joblib.dump(adaBoost, 'adaBoost.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorical\n",
    "\n",
    "def transformCategorical(data, categorical_columns, label_encoder, label_binarizer,dummy_columns):\n",
    "    data = data[categorical_columns].fillna(\"KOSONG\")\n",
    "    dummy_variables = pd.DataFrame([])\n",
    "    \n",
    "    j=0\n",
    "    for i in categorical_columns:\n",
    "        label_en = label_encoder[j]\n",
    "        label_bin = label_binarizer[j]\n",
    "        \n",
    "        encoded = label_en.transform(data[i])\n",
    "        binary = label_bin.transform(encoded)\n",
    "        \n",
    "        if binary.shape[1] == 1:\n",
    "            dummy = pd.DataFrame(binary, index = data.index)\n",
    "        else:\n",
    "            dummy = pd.DataFrame(binary, index = data.index)\n",
    "        \n",
    "        dummy_variables = pd.concat([dummy_variables, dummy], axis = 1)\n",
    "        j+=1\n",
    "    dummy_variables.columns = dummy_columns\n",
    "    \n",
    "    return dummy_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def validData(data and all necessary object)\n",
    "#\n",
    "# return clean_data\n",
    "#\n",
    "def validData(data, numerical_columns, categorical_columns, \n",
    "              imputer, label_encoder, label_binarizer, dummy_columns, \n",
    "              stemmer, lemmatizer, vectorizer, standardizer):\n",
    "    # preprocess numerical data using transformNumerical()\n",
    "    data_num_imputed = transformNumerical(data[numerical_columns], imputer)\n",
    "\n",
    "    # preprocess categorical data using transformCategorical()\n",
    "    data_cat_dummy = transformCategorical(data, categorical_columns, label_encoder, label_binarizer, dummy_columns) \n",
    "\n",
    "    # preprocess time data using preprocessTime()\n",
    "    data_time = preprocessTime(data['tweet_created'])\n",
    "\n",
    "    # preprocess coordinate data using preprocessCoordinate()\n",
    "    data_coord = preprocessCoordinate(data['tweet_coord'])\n",
    "\n",
    "    # preprocess text data using preprocessText()\n",
    "    data_text = preprocessText(data['text'], stemmer, lemmatizer)\n",
    "\n",
    "    # make feature_word using transformText()\n",
    "    text_feature = transformText(data_text, vectorizer)\n",
    "\n",
    "    # concat all data\n",
    "    data_concat = pd.concat([data_num_imputed, data_time, data_coord, text_feature], axis=1)\n",
    "\n",
    "    # standardize using transformStandardize()\n",
    "    data_standard = transformStandardize(data_concat, standardizer)\n",
    "    \n",
    "    data_valid = pd.concat([data_cat_dummy, data_standard], axis=1)\n",
    "    \n",
    "    return data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load necessary object\n",
    "# object = joblib.load(\"filename.pkl\")\n",
    "\n",
    "numerical_columns = joblib.load('numerical_col.pkl')\n",
    "categorical_columns = joblib.load('categorical_col.pkl')\n",
    "dummy_columns = joblib.load('dummy_columns.pkl')\n",
    "\n",
    "imputer = joblib.load('imputer.pkl')\n",
    "label_binarizer = joblib.load('label_binarizer.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "standardizer = joblib.load('standardizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess test data using validData()\n",
    "\n",
    "x_test_clean = validData(x_test, numerical_columns, categorical_columns, \n",
    "                         imputer, label_encoder, label_binarizer, dummy_columns, \n",
    "                         stemmer, lemmatizer, vectorizer, standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_American</th>\n",
       "      <th>airline_Delta</th>\n",
       "      <th>airline_Southwest</th>\n",
       "      <th>airline_US Airways</th>\n",
       "      <th>airline_United</th>\n",
       "      <th>airline_Virgin America</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>...</th>\n",
       "      <th>need</th>\n",
       "      <th>plane</th>\n",
       "      <th>servic</th>\n",
       "      <th>southwestair</th>\n",
       "      <th>thank</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "      <th>usairway</th>\n",
       "      <th>wa</th>\n",
       "      <th>wait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>-0.886816</td>\n",
       "      <td>1.625866</td>\n",
       "      <td>-0.541926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.961119</td>\n",
       "      <td>-0.061873</td>\n",
       "      <td>-0.716244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>3.574521</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>-0.886816</td>\n",
       "      <td>-0.811979</td>\n",
       "      <td>-0.600032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>-1.810784</td>\n",
       "      <td>-0.249400</td>\n",
       "      <td>1.143150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>3.223152</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>-0.221933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.108057</td>\n",
       "      <td>0.961119</td>\n",
       "      <td>-0.811979</td>\n",
       "      <td>-0.600032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214438</td>\n",
       "      <td>-0.218052</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>-0.416098</td>\n",
       "      <td>-0.343852</td>\n",
       "      <td>-0.253241</td>\n",
       "      <td>-0.543325</td>\n",
       "      <td>-0.464249</td>\n",
       "      <td>-0.312419</td>\n",
       "      <td>5.777006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_American  airline_Delta  airline_Southwest  airline_US Airways  \\\n",
       "8168                  0              1                  0                   0   \n",
       "13037                 1              0                  0                   0   \n",
       "8351                  0              1                  0                   0   \n",
       "8884                  0              1                  0                   0   \n",
       "13298                 1              0                  0                   0   \n",
       "\n",
       "       airline_United  airline_Virgin America  retweet_count       day  \\\n",
       "8168                0                       0      -0.108057 -0.886816   \n",
       "13037               0                       0      -0.108057  0.961119   \n",
       "8351                0                       0      -0.108057 -0.886816   \n",
       "8884                0                       0      -0.108057 -1.810784   \n",
       "13298               0                       0      -0.108057  0.961119   \n",
       "\n",
       "           hour    minute    ...         need     plane    servic  \\\n",
       "8168   1.625866 -0.541926    ...    -0.214438 -0.218052 -0.261428   \n",
       "13037 -0.061873 -0.716244    ...    -0.214438 -0.218052 -0.261428   \n",
       "8351  -0.811979 -0.600032    ...    -0.214438 -0.218052 -0.261428   \n",
       "8884  -0.249400  1.143150    ...    -0.214438 -0.218052 -0.261428   \n",
       "13298 -0.811979 -0.600032    ...    -0.214438 -0.218052 -0.261428   \n",
       "\n",
       "       southwestair     thank      time      unit  usairway        wa  \\\n",
       "8168      -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "13037     -0.416098 -0.343852 -0.253241 -0.543325 -0.464249  3.574521   \n",
       "8351      -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "8884      -0.416098  3.223152 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "13298     -0.416098 -0.343852 -0.253241 -0.543325 -0.464249 -0.312419   \n",
       "\n",
       "           wait  \n",
       "8168  -0.221933  \n",
       "13037 -0.221933  \n",
       "8351  -0.221933  \n",
       "8884  -0.221933  \n",
       "13298  5.777006  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top observation\n",
    "\n",
    "x_test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load classifier\n",
    "\n",
    "clf = joblib.load('randForest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70596346087556017"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate score\n",
    "\n",
    "clf.score(x_test_clean, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Single Raw Data (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictData(classifier, input_columns, numerical_columns, categorical_columns, \n",
    "                imputer, label_encoder, label_binarizer, dummy_columns, stemmer, \n",
    "                lemmatizer, vectorizer, standardizer):\n",
    "    raw_data = pd.Series([])\n",
    "    for i in range(0,len(input_columns)):\n",
    "        message = \"Masukkan nilai untuk kolom : \"+str(input_columns[i])+\" \"\n",
    "        raw_data[i] = input(message)\n",
    "    data = pd.DataFrame(raw_data)\n",
    "    data = data.transpose()\n",
    "    data.columns = input_columns\n",
    "    for i in numerical_columns :\n",
    "        data[i] = pd.to_numeric(data[i])\n",
    "    \n",
    "    # preprocess the data using validData()\n",
    "    data_clean = validData(data, numerical_columns, categorical_columns, \n",
    "                           imputer, label_encoder, label_binarizer, dummy_columns, \n",
    "                           stemmer, lemmatizer, vectorizer, standardizer)\n",
    "    \n",
    "    result = classifier.predict(data_clean)\n",
    "    \n",
    "    print(\"The sentiment is \" + result)\n",
    "    print(classifier.predict_proba(data_clean))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load necessary object\n",
    "# object = joblib.load(\"filename.pkl\")\n",
    "\n",
    "classifier = joblib.load('randForest.pkl')\n",
    "input_columns = joblib.load('input_col.pkl')\n",
    "numerical_columns = joblib.load('numerical_col.pkl')\n",
    "categorical_columns = joblib.load('categorical_col.pkl')\n",
    "dummy_columns = joblib.load('dummy_columns.pkl')\n",
    "\n",
    "imputer = joblib.load('imputer.pkl')\n",
    "label_binarizer = joblib.load('label_binarizer.pkl')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "standardizer = joblib.load('standardizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misal, akan dilakukan prediksi untuk data berikut :<br>\n",
    "<table>\n",
    "<tr> <td> airline_sentiment_confidence </td><td>             1 <td></td></tr>\n",
    "<tr> <td>negativereason      </td><td>    Lost Luggage</td></tr>\n",
    "<tr> <td>negativereason_confidence </td><td> 1</td></tr>\n",
    "<tr> <td>airline </td><td>  Virgin America</td></tr>\n",
    "<tr> <td>retweet_count </td><td>  0</td></tr>\n",
    "<tr> <td>text </td><td> @VirginAmerica everything was fine until you lost my bag</td></tr>\n",
    "<tr> <td>tweet_coord </td><td>  [40.6413712, -73.78311558]</td></tr>\n",
    "<tr> <td>tweet_created </td><td>  23-02-15 13:08:00 -0800</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masukkan nilai untuk kolom : airline Virgin America\n",
      "Masukkan nilai untuk kolom : retweet_count 0\n",
      "Masukkan nilai untuk kolom : text @VirginAmerica everything was fine until you lost my bag\n",
      "Masukkan nilai untuk kolom : tweet_coord [40.6413712, -73.78311558]\n",
      "Masukkan nilai untuk kolom : tweet_created 23-02-15 13:08:00 -0800\n",
      "['The sentiment is negative']\n",
      "[[ 0.52680993  0.23009963  0.24309044]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call predictData()\n",
    "\n",
    "predictData(classifier, input_columns, numerical_columns, categorical_columns, \n",
    "                imputer, label_encoder, label_binarizer, dummy_columns, stemmer, \n",
    "                lemmatizer, vectorizer, standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
